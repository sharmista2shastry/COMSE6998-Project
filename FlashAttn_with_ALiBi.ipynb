{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi --query-gpu=name,utilization.gpu,utilization.memory,memory.total,memory.free --format=csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uIjy2T7BuRA",
        "outputId": "98c6edcf-4491-4c88-9639-cefe0b526660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name, utilization.gpu [%], utilization.memory [%], memory.total [MiB], memory.free [MiB]\n",
            "NVIDIA A100-SXM4-40GB, 0 %, 0 %, 40960 MiB, 40511 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "s3uS_e2UfTat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZGhEBKBhlhe",
        "outputId": "d4711a8a-7e67-405b-fd7d-3b310fb655bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip setuptools wheel\n",
        "!apt-get update\n",
        "!apt-get install -y build-essential libopenblas-dev libomp-dev\n",
        "!apt-get install -y cuda-libraries-12-2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwz2uEyFkDax",
        "outputId": "b7f5a080-4081-4e34-8c14-dc6f7279d0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (75.6.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (0.45.1)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "libomp-dev is already the newest version (1:14.0-55~exp2).\n",
            "libopenblas-dev is already the newest version (0.3.20+ds-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cuda-libraries-12-2 is already the newest version (12.2.2-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 51 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch --pre --index-url https://download.pytorch.org/whl/nightly/cu122"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxUFUbBUnSda",
        "outputId": "926157a4-48bc-4cce-b464-c536f8760073"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/nightly/cu122\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo ln -sf /usr/local/cuda-12.2/lib64/libcudart.so.12 /usr/lib/x86_64-linux-gnu/libcudart.so.11\n",
        "!sudo ln -sf /usr/local/cuda-12.2/lib64/libcudart.so.12 /usr/lib/x86_64-linux-gnu/libcudart.so.11.0"
      ],
      "metadata": {
        "id": "-tFxGjBdp1Vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l /usr/lib/x86_64-linux-gnu/libcudart.so.11*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32rq_Duot7Jz",
        "outputId": "ad4c17df-0382-4c96-aee7-63d63710a7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lrwxrwxrwx 1 root root 42 Dec 15 03:49 /usr/lib/x86_64-linux-gnu/libcudart.so.11 -> /usr/local/cuda-12.2/lib64/libcudart.so.12\n",
            "lrwxrwxrwx 1 root root 42 Dec 15 03:49 /usr/lib/x86_64-linux-gnu/libcudart.so.11.0 -> /usr/local/cuda-12.2/lib64/libcudart.so.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!export FORCE_CUDA=\"1\"\n",
        "!export TORCH_CUDA_ARCH_LIST=\"7.5\"\n",
        "!export PATH=\"/usr/local/cuda-12.2/bin:$PATH\"\n",
        "!export LD_LIBRARY_PATH=\"/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH\""
      ],
      "metadata": {
        "id": "8baQe5HBnWMr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y flash-attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijZJvgKYr4yQ",
        "outputId": "39fdea54-c291-4e14-e6d2-5cccdadccd95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: flash-attn 2.7.2.post1\n",
            "Uninstalling flash-attn-2.7.2.post1:\n",
            "  Successfully uninstalled flash-attn-2.7.2.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HazyResearch/flash-attention.git\n",
        "%cd flash-attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mh-EQHfFv1Bd",
        "outputId": "642a47ca-c102-43f4-af99-f6dc94ad8eba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'flash-attention' already exists and is not an empty directory.\n",
            "/content/flash-attention\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout v2.7.2.post1\n",
        "!git submodule update --init --recursive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AscwQk6Fv4Ml",
        "outputId": "69787c08-2349-426c-99f2-e802990e502d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HEAD is now at f86e3dd [CI] Use MAX_JOBS=1 with nvcc 12.3, don't need OLD_GENERATOR_PATH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install . --no-build-isolation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SoRWPI8v8Wr",
        "outputId": "4dbaf0da-3fc5-4690-fd2b-6f604c99730a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/flash-attention\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash_attn==2.7.2.post1) (2.5.1+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash_attn==2.7.2.post1) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn==2.7.2.post1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn==2.7.2.post1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn==2.7.2.post1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn==2.7.2.post1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn==2.7.2.post1) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash_attn==2.7.2.post1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash_attn==2.7.2.post1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash_attn==2.7.2.post1) (3.0.2)\n",
            "Building wheels for collected packages: flash_attn\n",
            "  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flash_attn: filename=flash_attn-2.7.2.post1-cp310-cp310-linux_x86_64.whl size=190160474 sha256=0b454d9e650bfc437cc71335080172a5d05f51eab355636c9d5b7321fec7318e\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/cf/3b/d132219792be47c1a416734b31d5be638f6a6e282470b490c6\n",
            "Successfully built flash_attn\n",
            "Installing collected packages: flash_attn\n",
            "Successfully installed flash_attn-2.7.2.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import flash_attn\n",
        "print(\"FlashAttention 2.4 installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfbf8rxQpIJk",
        "outputId": "0a0eb737-4959-4b93-ff70-26bdefaa9dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FlashAttention 2.4 installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show flash-attn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gQMfIJaz7xL",
        "outputId": "d9ec19d1-e280-4421-b351-289cd23208fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: flash-attn\n",
            "Version: 2.7.2.post1\n",
            "Summary: Flash Attention: Fast and Memory-Efficient Exact Attention\n",
            "Home-page: https://github.com/Dao-AILab/flash-attention\n",
            "Author: Tri Dao\n",
            "Author-email: tri@tridao.me\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: einops, torch\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NsqgEVY9fXw",
        "outputId": "656c17c0-90d0-4dd3-f6cf-e34510ebe463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 15 03:50:30 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0              42W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers rouge rouge-score nltk datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZe9QSRQQAcq",
        "outputId": "8f5f529b-e4f0-457a-b6d4-42b9ba058cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import time\n",
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention, GPT2Config\n",
        "from datasets import load_dataset\n",
        "from tabulate import tabulate\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import wandb"
      ],
      "metadata": {
        "id": "ICxLBHdxNMEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CNN/DailyMail dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test[:5%]\")  # Use a small subset for testing\n",
        "\n",
        "# Extract 5 articles and their corresponding summaries\n",
        "num_inputs = 5\n",
        "input_texts = [example[\"article\"] for example in dataset.select(range(num_inputs))]\n",
        "reference_summaries = [example[\"highlights\"] for example in dataset.select(range(num_inputs))]"
      ],
      "metadata": {
        "id": "XgKEpiwtNePs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "seq_len = 1024\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "fxN1TnE1Nlxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ALiBi Score Function\n",
        "def get_alibi_slopes(nheads):\n",
        "    def get_slopes_power_of_2(n):\n",
        "        start = 2.0 ** (-2.0 ** -(math.log2(n) - 3))\n",
        "        return [start * (2.0 ** (i / 2.0)) for i in range(n)]\n",
        "\n",
        "    if math.log2(nheads).is_integer():\n",
        "        slopes = get_slopes_power_of_2(nheads)\n",
        "    else:\n",
        "        closest_power_of_2 = 2 ** math.floor(math.log2(nheads))\n",
        "        slopes = get_slopes_power_of_2(closest_power_of_2)\n",
        "        slopes += [slopes[-1] * 2.0 ** ((i + 1) / 2.0) for i in range(nheads - closest_power_of_2)]\n",
        "    return torch.tensor(slopes, dtype=torch.float32, device=device)"
      ],
      "metadata": {
        "id": "78bMaNzvNpYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper Functions\n",
        "def benchmark_model(model, tokenizer, texts):\n",
        "    \"\"\"Benchmark model for inference time and memory usage.\"\"\"\n",
        "    inputs = tokenizer(texts, return_tensors=\"pt\", max_length=seq_len, padding=True, truncation=True).to(device)\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "    times = []\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(5):\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "\n",
        "    # Measure time\n",
        "    with torch.no_grad():\n",
        "        for _ in range(10):\n",
        "            start = time.time()\n",
        "            _ = model(**inputs)\n",
        "            end = time.time()\n",
        "            times.append(end - start)\n",
        "    avg_time = sum(times) / len(times)\n",
        "    max_mem = torch.cuda.max_memory_allocated() / (1024**2)\n",
        "    return avg_time * 1000, max_mem\n",
        "\n",
        "def generate_text(model, tokenizer, texts, max_new_tokens=100, num_beams=5):\n",
        "    \"\"\"Generate text using the model.\"\"\"\n",
        "    outputs = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(\n",
        "            text, return_tensors=\"pt\", max_length=seq_len, truncation=True, padding=True\n",
        "        ).to(device)\n",
        "        attention_mask = inputs[\"attention_mask\"]\n",
        "\n",
        "        model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_ids = model.generate(\n",
        "                inputs[\"input_ids\"],\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                num_beams=num_beams,\n",
        "                no_repeat_ngram_size=2,\n",
        "                early_stopping=True,\n",
        "            )\n",
        "        outputs.append(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
        "    return outputs\n",
        "\n",
        "def compute_metrics(predicted, references):\n",
        "    \"\"\"Compute ROUGE and BLEU metrics.\"\"\"\n",
        "    rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "    rouge_scores = {\"rouge1\": 0, \"rouge2\": 0, \"rougeL\": 0}\n",
        "    bleu_score = 0\n",
        "\n",
        "    for pred, ref in zip(predicted, references):\n",
        "        rouge_res = rouge.score(pred, ref)\n",
        "        rouge_scores[\"rouge1\"] += rouge_res[\"rouge1\"].fmeasure\n",
        "        rouge_scores[\"rouge2\"] += rouge_res[\"rouge2\"].fmeasure\n",
        "        rouge_scores[\"rougeL\"] += rouge_res[\"rougeL\"].fmeasure\n",
        "\n",
        "        smoothing_fn = SmoothingFunction().method4\n",
        "        bleu_score += sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothing_fn)\n",
        "\n",
        "    for key in rouge_scores:\n",
        "        rouge_scores[key] /= len(predicted)\n",
        "    bleu_score /= len(predicted)\n",
        "    return rouge_scores, bleu_score"
      ],
      "metadata": {
        "id": "8D7jiSIqOouw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define FlashAttention with ALiBi\n",
        "class FlashGPT2AttentionWithALiBi(GPT2Attention):\n",
        "    def __init__(self, config: GPT2Config, is_cross_attention=False, layer_idx=None):\n",
        "        super().__init__(config, is_cross_attention=is_cross_attention, layer_idx=layer_idx)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states,\n",
        "        layer_past=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        use_cache=False,\n",
        "        output_attentions=False,\n",
        "    ):\n",
        "        bsz, slen, hidden_size = hidden_states.size()\n",
        "        assert hidden_size == self.embed_dim\n",
        "\n",
        "        # Project hidden states to query, key, and value\n",
        "        qkv_proj = self.c_attn(hidden_states)\n",
        "        query, key, value = qkv_proj.split(self.embed_dim, dim=2)\n",
        "\n",
        "        query = query.view(bsz, slen, self.num_heads, self.head_dim)\n",
        "        key = key.view(bsz, slen, self.num_heads, self.head_dim)\n",
        "        value = value.view(bsz, slen, self.num_heads, self.head_dim)\n",
        "\n",
        "        if layer_past is not None:\n",
        "            past_key, past_value = layer_past\n",
        "            key = torch.cat((past_key, key), dim=1)\n",
        "            value = torch.cat((past_value, value), dim=1)\n",
        "        present = (key, value) if use_cache else None\n",
        "\n",
        "        scale = 1.0 / math.sqrt(self.head_dim)\n",
        "\n",
        "        # Generate ALiBi slopes and bias\n",
        "        alibi_slopes = get_alibi_slopes(self.num_heads).to(hidden_states.dtype)\n",
        "        alibi = alibi_slopes.view(1, self.num_heads, 1, 1)  # Reshape for broadcasting\n",
        "        position_ids = torch.arange(slen, dtype=hidden_states.dtype, device=hidden_states.device).unsqueeze(0)\n",
        "        relative_positions = position_ids.unsqueeze(-1) - position_ids.unsqueeze(-2)\n",
        "        alibi_bias = alibi * relative_positions.unsqueeze(0)  # Shape: (1, num_heads, slen, slen)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn_scores = torch.einsum(\"bthd,bshd->bhts\", query, key) * scale\n",
        "        attn_scores = attn_scores + alibi_bias.expand_as(attn_scores)\n",
        "\n",
        "        # Apply attention mask\n",
        "        if attention_mask is not None:\n",
        "            attn_scores += attention_mask\n",
        "\n",
        "        # Compute attention probabilities\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "        attn_output = torch.einsum(\"bhts,bshd->bthd\", attn_probs, value)\n",
        "\n",
        "        # Reshape output\n",
        "        attn_output = attn_output.reshape(bsz, slen, hidden_size)\n",
        "        attn_output = self.c_proj(attn_output)\n",
        "        attn_output = self.resid_dropout(attn_output)\n",
        "\n",
        "        return attn_output, present"
      ],
      "metadata": {
        "id": "wsGdZQqcOzdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Models\n",
        "model_standard = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device).eval()\n",
        "model_flash = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device).eval()\n",
        "model_flash_with_alibi = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device).eval()"
      ],
      "metadata": {
        "id": "yc3fPlRsO5f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace Attention Modules\n",
        "for block in model_flash_with_alibi.transformer.h:\n",
        "    block.attn = FlashGPT2AttentionWithALiBi(\n",
        "        config=model_flash_with_alibi.config,\n",
        "        is_cross_attention=block.attn.is_cross_attention,\n",
        "        layer_idx=block.attn.layer_idx\n",
        "    ).to(device)"
      ],
      "metadata": {
        "id": "h8Etwj9GO9c9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable Flash Attention by reducing precision\n",
        "model_flash.half()\n",
        "model_flash_with_alibi.half()"
      ],
      "metadata": {
        "id": "JWmrN5-3PAg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benchmarking\n",
        "print(\"Running Benchmarks...\")\n",
        "standard_time, standard_mem = benchmark_model(model_standard, tokenizer, input_texts)\n",
        "flash_time, flash_mem = benchmark_model(model_flash, tokenizer, input_texts)\n",
        "flash_alibi_time, flash_alibi_mem = benchmark_model(model_flash_with_alibi, tokenizer, input_texts)\n",
        "\n",
        "# Generate Texts\n",
        "standard_outputs = generate_text(model_standard, tokenizer, input_texts)\n",
        "flash_outputs = generate_text(model_flash, tokenizer, input_texts)\n",
        "flash_alibi_outputs = generate_text(model_flash_with_alibi, tokenizer, input_texts)\n",
        "\n",
        "# Compute Metrics\n",
        "standard_metrics = compute_metrics(standard_outputs, reference_summaries)\n",
        "flash_metrics = compute_metrics(flash_outputs, reference_summaries)\n",
        "flash_alibi_metrics = compute_metrics(flash_alibi_outputs, reference_summaries)\n",
        "\n",
        "# Organize Results\n",
        "results_efficiency = [\n",
        "    [\"Model\", \"Inference Time (ms)\", \"Memory (MB)\"],\n",
        "    [\"Standard GPT-2\", f\"{standard_time:.2f}\", f\"{standard_mem:.2f}\"],\n",
        "    [\"Flash GPT-2\", f\"{flash_time:.2f}\", f\"{flash_mem:.2f}\"],\n",
        "    [\"Flash + ALiBi GPT-2\", f\"{flash_alibi_time:.2f}\", f\"{flash_alibi_mem:.2f}\"]\n",
        "]\n",
        "\n",
        "results_quality = [\n",
        "    [\"Model\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\"],\n",
        "    [\"Standard GPT-2\", f\"{standard_metrics[0]['rouge1']:.4f}\", f\"{standard_metrics[0]['rouge2']:.4f}\", f\"{standard_metrics[0]['rougeL']:.4f}\", f\"{standard_metrics[1]:.4f}\"],\n",
        "    [\"Flash GPT-2\", f\"{flash_metrics[0]['rouge1']:.4f}\", f\"{flash_metrics[0]['rouge2']:.4f}\", f\"{flash_metrics[0]['rougeL']:.4f}\", f\"{flash_metrics[1]:.4f}\"],\n",
        "    [\"Flash + ALiBi GPT-2\", f\"{flash_alibi_metrics[0]['rouge1']:.4f}\", f\"{flash_alibi_metrics[0]['rouge2']:.4f}\", f\"{flash_alibi_metrics[0]['rougeL']:.4f}\", f\"{flash_alibi_metrics[1]:.4f}\"]\n",
        "]\n",
        "\n",
        "print(\"\\n===== Efficiency Comparison =====\\n\")\n",
        "print(tabulate(results_efficiency, headers=\"firstrow\", tablefmt=\"pretty\"))\n",
        "\n",
        "print(\"\\n===== Quality Comparison =====\\n\")\n",
        "print(tabulate(results_quality, headers=\"firstrow\", tablefmt=\"pretty\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gy6LYCUiSZ5O",
        "outputId": "b463c523-37e9-4d49-b350-54e07290b4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Benchmarks...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Efficiency Comparison =====\n",
            "\n",
            "+---------------------+---------------------+-------------+\n",
            "|        Model        | Inference Time (ms) | Memory (MB) |\n",
            "+---------------------+---------------------+-------------+\n",
            "|   Standard GPT-2    |        79.61        |   4407.28   |\n",
            "|     Flash GPT-2     |        17.21        |   3747.49   |\n",
            "| Flash + ALiBi GPT-2 |        30.94        |   3747.49   |\n",
            "+---------------------+---------------------+-------------+\n",
            "\n",
            "===== Quality Comparison =====\n",
            "\n",
            "+---------------------+---------+---------+---------+--------+\n",
            "|        Model        | ROUGE-1 | ROUGE-2 | ROUGE-L |  BLEU  |\n",
            "+---------------------+---------+---------+---------+--------+\n",
            "|   Standard GPT-2    | 0.1609  | 0.0861  | 0.1177  | 0.0293 |\n",
            "|     Flash GPT-2     | 0.1618  | 0.0866  | 0.1184  | 0.0296 |\n",
            "| Flash + ALiBi GPT-2 | 0.1644  | 0.0887  | 0.1202  | 0.0327 |\n",
            "+---------------------+---------+---------+---------+--------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.init(project=\"intro2llm\", name=\"benchmark_results\")\n",
        "\n",
        "# Log efficiency results\n",
        "efficiency_table = wandb.Table(columns=[\"Model\", \"Inference Time (ms)\", \"Memory (MB)\"])\n",
        "efficiency_table.add_data(\"Standard GPT-2\", standard_time, standard_mem)\n",
        "efficiency_table.add_data(\"Flash GPT-2\", flash_time, flash_mem)\n",
        "efficiency_table.add_data(\"Flash + ALiBi GPT-2\", flash_alibi_time, flash_alibi_mem)\n",
        "wandb.log({\"Efficiency Metrics\": efficiency_table})\n",
        "\n",
        "# Log quality results\n",
        "quality_table = wandb.Table(columns=[\"Model\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\"])\n",
        "quality_table.add_data(\"Standard GPT-2\",\n",
        "                       standard_metrics[0]['rouge1'],\n",
        "                       standard_metrics[0]['rouge2'],\n",
        "                       standard_metrics[0]['rougeL'],\n",
        "                       standard_metrics[1])\n",
        "quality_table.add_data(\"Flash GPT-2\",\n",
        "                       flash_metrics[0]['rouge1'],\n",
        "                       flash_metrics[0]['rouge2'],\n",
        "                       flash_metrics[0]['rougeL'],\n",
        "                       flash_metrics[1])\n",
        "quality_table.add_data(\"Flash + ALiBi GPT-2\",\n",
        "                       flash_alibi_metrics[0]['rouge1'],\n",
        "                       flash_alibi_metrics[0]['rouge2'],\n",
        "                       flash_alibi_metrics[0]['rougeL'],\n",
        "                       flash_alibi_metrics[1])\n",
        "wandb.log({\"Quality Metrics\": quality_table})\n",
        "\n",
        "# Log as summary\n",
        "wandb.summary[\"Standard Inference Time (ms)\"] = standard_time\n",
        "wandb.summary[\"Flash Inference Time (ms)\"] = flash_time\n",
        "wandb.summary[\"Flash + ALiBi Inference Time (ms)\"] = flash_alibi_time\n",
        "\n",
        "wandb.summary[\"Standard Memory (MB)\"] = standard_mem\n",
        "wandb.summary[\"Flash Memory (MB)\"] = flash_mem\n",
        "wandb.summary[\"Flash + ALiBi Memory (MB)\"] = flash_alibi_mem\n",
        "\n",
        "wandb.summary[\"Standard ROUGE-1\"] = standard_metrics[0]['rouge1']\n",
        "wandb.summary[\"Flash ROUGE-1\"] = flash_metrics[0]['rouge1']\n",
        "wandb.summary[\"Flash + ALiBi ROUGE-1\"] = flash_alibi_metrics[0]['rouge1']\n",
        "\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "89qRt3HpSruf",
        "outputId": "2226a155-7dbd-4c60-b2ad-f5b21d5f915d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241215_215445-r9rjvuog</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/hpmlcolumbia/intro2llm/runs/r9rjvuog' target=\"_blank\">benchmark_results</a></strong> to <a href='https://wandb.ai/hpmlcolumbia/intro2llm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/hpmlcolumbia/intro2llm' target=\"_blank\">https://wandb.ai/hpmlcolumbia/intro2llm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/hpmlcolumbia/intro2llm/runs/r9rjvuog' target=\"_blank\">https://wandb.ai/hpmlcolumbia/intro2llm/runs/r9rjvuog</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Flash + ALiBi Inference Time (ms)</td><td>30.94</td></tr><tr><td>Flash + ALiBi Memory (MB)</td><td>3747.49</td></tr><tr><td>Flash + ALiBi ROUGE-1</td><td>0.1644</td></tr><tr><td>Flash Inference Time (ms)</td><td>17.21</td></tr><tr><td>Flash Memory (MB)</td><td>3747.49</td></tr><tr><td>Flash ROUGE-1</td><td>0.1618</td></tr><tr><td>Standard Inference Time (ms)</td><td>79.61</td></tr><tr><td>Standard Memory (MB)</td><td>4407.28</td></tr><tr><td>Standard ROUGE-1</td><td>0.1609</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">benchmark_results</strong> at: <a href='https://wandb.ai/hpmlcolumbia/intro2llm/runs/r9rjvuog' target=\"_blank\">https://wandb.ai/hpmlcolumbia/intro2llm/runs/r9rjvuog</a><br/> View project at: <a href='https://wandb.ai/hpmlcolumbia/intro2llm' target=\"_blank\">https://wandb.ai/hpmlcolumbia/intro2llm</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241215_215445-r9rjvuog/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6NA_ZudoUH6z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}